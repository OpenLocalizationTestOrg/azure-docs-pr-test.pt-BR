---
title: "Analisar os logs do Application Insight com Spark – Azure HDInsight | Microsoft Docs"
description: Saiba como exportar logs do Application Insight para o armazenamento de blobs e, em seguida, analise os logs com Spark no HDInsight.
services: hdinsight
documentationcenter: 
author: Blackmist
manager: jhubbard
editor: cgronlun
ms.assetid: 883beae6-9839-45b5-94f7-7eb0f4534ad5
ms.service: hdinsight
ms.custom: hdinsightactive
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 08/15/2017
ms.author: larryfr
ms.openlocfilehash: d98e403683618ef6115372f99e4949af87af4490
ms.sourcegitcommit: 50e23e8d3b1148ae2d36dad3167936b4e52c8a23
ms.translationtype: MT
ms.contentlocale: pt-BR
ms.lasthandoff: 08/18/2017
---
# <a name="analyze-application-insights-telemetry-logs-with-spark-on-hdinsight"></a><span data-ttu-id="4048c-103">Analisar logs de telemetria do Application Insights com Spark no HDInsight</span><span class="sxs-lookup"><span data-stu-id="4048c-103">Analyze Application Insights telemetry logs with Spark on HDInsight</span></span>

<span data-ttu-id="4048c-104">Aprenda a usar o Spark no HDInsight para analisar dados de telemetria do Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-104">Learn how to use Spark on HDInsight to analyze Application Insight telemetry data.</span></span>

<span data-ttu-id="4048c-105">[Visual Studio Application Insights](../application-insights/app-insights-overview.md) é um serviço de análise que monitora seus aplicativos Web.</span><span class="sxs-lookup"><span data-stu-id="4048c-105">[Visual Studio Application Insights](../application-insights/app-insights-overview.md) is an analytics service that monitors your web applications.</span></span> <span data-ttu-id="4048c-106">Os dados de telemetria gerados pelo Application Insights podem ser exportados para o armazenamento do Azure.</span><span class="sxs-lookup"><span data-stu-id="4048c-106">Telemetry data generated by Application Insights can be exported to Azure Storage.</span></span> <span data-ttu-id="4048c-107">Depois que os dados estiverem no armazenamento do Azure, o HDInsight pode ser usado para analisá-los.</span><span class="sxs-lookup"><span data-stu-id="4048c-107">Once the data is in Azure Storage, HDInsight can be used to analyze it.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="4048c-108">Pré-requisitos</span><span class="sxs-lookup"><span data-stu-id="4048c-108">Prerequisites</span></span>

* <span data-ttu-id="4048c-109">Um aplicativo que está configurado para usar o Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-109">An application that is configured to use Application Insights.</span></span>

* <span data-ttu-id="4048c-110">Familiaridade com a criação de um cluster HDInsight baseado em Linux.</span><span class="sxs-lookup"><span data-stu-id="4048c-110">Familiarity with creating a Linux-based HDInsight cluster.</span></span> <span data-ttu-id="4048c-111">Para saber mais, veja [Criar o Spark no HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="4048c-111">For more information, see [Create Spark on HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

  > [!IMPORTANT]
  > <span data-ttu-id="4048c-112">As etapas deste documento exigem um cluster HDInsight que usa Linux.</span><span class="sxs-lookup"><span data-stu-id="4048c-112">The steps in this document require an HDInsight cluster that uses Linux.</span></span> <span data-ttu-id="4048c-113">O Linux é o único sistema operacional usado no HDInsight versão 3.4 ou superior.</span><span class="sxs-lookup"><span data-stu-id="4048c-113">Linux is the only operating system used on HDInsight version 3.4 or greater.</span></span> <span data-ttu-id="4048c-114">Para obter mais informações, confira [baixa do HDInsight no Windows](hdinsight-component-versioning.md#hdinsight-windows-retirement).</span><span class="sxs-lookup"><span data-stu-id="4048c-114">For more information, see [HDInsight retirement on Windows](hdinsight-component-versioning.md#hdinsight-windows-retirement).</span></span>

* <span data-ttu-id="4048c-115">Um navegador da Web.</span><span class="sxs-lookup"><span data-stu-id="4048c-115">A web browser.</span></span>

<span data-ttu-id="4048c-116">Os recursos a seguir foram usados para desenvolvimento e testes deste documento:</span><span class="sxs-lookup"><span data-stu-id="4048c-116">The following resources were used in developing and testing this document:</span></span>

* <span data-ttu-id="4048c-117">Os dados de telemetria do Application Insights foram gerados usando um [aplicativo Web do Node.js configurado para usar o Application Insights](../application-insights/app-insights-nodejs.md).</span><span class="sxs-lookup"><span data-stu-id="4048c-117">Application Insights telemetry data was generated using a [Node.js web app configured to use Application Insights](../application-insights/app-insights-nodejs.md).</span></span>

* <span data-ttu-id="4048c-118">Um cluster Spark no HDInsight versão 3.5 baseado em Linux foi usado para analisar os dados.</span><span class="sxs-lookup"><span data-stu-id="4048c-118">A Linux-based Spark on HDInsight cluster version 3.5 was used to analyze the data.</span></span>

## <a name="architecture-and-planning"></a><span data-ttu-id="4048c-119">Arquitetura e planejamento</span><span class="sxs-lookup"><span data-stu-id="4048c-119">Architecture and planning</span></span>

<span data-ttu-id="4048c-120">O diagrama a seguir ilustra a arquitetura de serviço deste exemplo:</span><span class="sxs-lookup"><span data-stu-id="4048c-120">The following diagram illustrates the service architecture of this example:</span></span>

![diagrama mostrando dados que fluem do Application Insights para o armazenamento de blobs e, em seguida, são processados pelo Spark no HDInsight](./media/hdinsight-spark-analyze-application-insight-logs/appinsightshdinsight.png)

### <a name="azure-storage"></a><span data-ttu-id="4048c-122">Armazenamento do Azure</span><span class="sxs-lookup"><span data-stu-id="4048c-122">Azure storage</span></span>

<span data-ttu-id="4048c-123">O Application Insights pode ser configurado para exportar informações de telemetria para blobs continuamente.</span><span class="sxs-lookup"><span data-stu-id="4048c-123">Application Insights can be configured to continuously export telemetry information to blobs.</span></span> <span data-ttu-id="4048c-124">O HDInsight poderá então ler os dados armazenados em blobs.</span><span class="sxs-lookup"><span data-stu-id="4048c-124">HDInsight can then read data stored in the blobs.</span></span> <span data-ttu-id="4048c-125">No entanto, há alguns requisitos que devem ser seguidos:</span><span class="sxs-lookup"><span data-stu-id="4048c-125">However, there are some requirements that you must follow:</span></span>

* <span data-ttu-id="4048c-126">**Local**: se a conta de armazenamento e o HDInsight estiverem em locais diferentes, isso poderá aumentar a latência.</span><span class="sxs-lookup"><span data-stu-id="4048c-126">**Location**: If the Storage Account and HDInsight are in different locations, it may increase latency.</span></span> <span data-ttu-id="4048c-127">Isso também aumenta o custo, pois os encargos são aplicados a dados sendo transferidos entre regiões.</span><span class="sxs-lookup"><span data-stu-id="4048c-127">It also increases cost, as egress charges are applied to data moving between regions.</span></span>

    > [!WARNING]
    > <span data-ttu-id="4048c-128">Não há suporte para o uso de uma Conta de armazenamento em um local diferente do HDInsight.</span><span class="sxs-lookup"><span data-stu-id="4048c-128">Using a Storage Account in a different location than HDInsight is not supported.</span></span>

* <span data-ttu-id="4048c-129">**Tipo de Blob**: o HDInsight dá suporte apenas aos blobs de bloco.</span><span class="sxs-lookup"><span data-stu-id="4048c-129">**Blob type**: HDInsight only supports block blobs.</span></span> <span data-ttu-id="4048c-130">O Application Insights usa blobs de bloco como padrão, então, deve funcionar por padrão com o HDInsight.</span><span class="sxs-lookup"><span data-stu-id="4048c-130">Application Insights defaults to using block blobs, so should work by default with HDInsight.</span></span>

<span data-ttu-id="4048c-131">Para obter informações sobre como adicionar armazenamento adicional a um cluster HDInsight existente, consulte o documento [Adicionar contas de armazenamento adicionais](hdinsight-hadoop-add-storage.md).</span><span class="sxs-lookup"><span data-stu-id="4048c-131">For information on adding additional storage to an existing HDInsight cluster, see the [Add additional storage accounts](hdinsight-hadoop-add-storage.md) document.</span></span>

### <a name="data-schema"></a><span data-ttu-id="4048c-132">Esquema de dados</span><span class="sxs-lookup"><span data-stu-id="4048c-132">Data schema</span></span>

<span data-ttu-id="4048c-133">O Application Insights fornece informações para [exportar o modelo de dados](../application-insights/app-insights-export-data-model.md) para o formato de dados de telemetria exportados para os blobs.</span><span class="sxs-lookup"><span data-stu-id="4048c-133">Application Insights provides [export data model](../application-insights/app-insights-export-data-model.md) information for the telemetry data format exported to blobs.</span></span> <span data-ttu-id="4048c-134">As etapas neste documento usam Spark SQL para trabalhar com os dados.</span><span class="sxs-lookup"><span data-stu-id="4048c-134">The steps in this document use Spark SQL to work with the data.</span></span> <span data-ttu-id="4048c-135">O Spark SQL pode gerar automaticamente um esquema para a estrutura de dados JSON registrada pelo Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-135">Spark SQL can automatically generate a schema for the JSON data structure logged by Application Insights.</span></span>

## <a name="export-telemetry-data"></a><span data-ttu-id="4048c-136">Exportar dados de telemetria</span><span class="sxs-lookup"><span data-stu-id="4048c-136">Export telemetry data</span></span>

<span data-ttu-id="4048c-137">Siga as etapas em [Configurar a exportação contínua](../application-insights/app-insights-export-telemetry.md) para configurar o Application Insights para exportar informações de telemetria para um blob de armazenamento do Azure.</span><span class="sxs-lookup"><span data-stu-id="4048c-137">Follow the steps in [Configure Continuous Export](../application-insights/app-insights-export-telemetry.md) to configure your Application Insights to export telemetry information to an Azure storage blob.</span></span>

## <a name="configure-hdinsight-to-access-the-data"></a><span data-ttu-id="4048c-138">Configurar o HDInsight para acessar os dados</span><span class="sxs-lookup"><span data-stu-id="4048c-138">Configure HDInsight to access the data</span></span>

<span data-ttu-id="4048c-139">Se você estiver criando um cluster HDInsight, adicione a conta de armazenamento durante a criação do cluster.</span><span class="sxs-lookup"><span data-stu-id="4048c-139">If you are creating an HDInsight cluster, add the storage account during cluster creation.</span></span>

<span data-ttu-id="4048c-140">Para adicionar a Conta de armazenamento do Azure a um cluster existente, use as informações no documento [Adicionar contas de armazenamento adicionais](hdinsight-hadoop-add-storage.md).</span><span class="sxs-lookup"><span data-stu-id="4048c-140">To add the Azure Storage Account to an existing cluster, use the information in the [Add additional Storage Accounts](hdinsight-hadoop-add-storage.md) document.</span></span>

## <a name="analyze-the-data-pyspark"></a><span data-ttu-id="4048c-141">Analisar os dados: PySpark</span><span class="sxs-lookup"><span data-stu-id="4048c-141">Analyze the data: PySpark</span></span>

1. <span data-ttu-id="4048c-142">No [Portal do Azure](https://portal.azure.com), selecione o Spark no cluster HDInsight.</span><span class="sxs-lookup"><span data-stu-id="4048c-142">From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster.</span></span> <span data-ttu-id="4048c-143">Na seção **Links Rápidos**, selecione **Painéis do Cluster**, em seguida, selecione **Notebook Jupyter** na folha Painel do Cluster__.</span><span class="sxs-lookup"><span data-stu-id="4048c-143">From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.</span></span>

    ![Os painéis de cluster](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)

2. <span data-ttu-id="4048c-145">No canto superior direito da página Jupyter, selecione **Novo**, então, **PySpark**.</span><span class="sxs-lookup"><span data-stu-id="4048c-145">In the upper right corner of the Jupyter page, select **New**, and then **PySpark**.</span></span> <span data-ttu-id="4048c-146">Uma nova guia do navegador que contém um Bloco de Notas Jupyter com base em Python é aberta.</span><span class="sxs-lookup"><span data-stu-id="4048c-146">A new browser tab containing a Python-based Jupyter Notebook opens.</span></span>

3. <span data-ttu-id="4048c-147">No primeiro campo (denominado **célula**) na página, digite o seguinte texto:</span><span class="sxs-lookup"><span data-stu-id="4048c-147">In the first field (called a **cell**) on the page, enter the following text:</span></span>

   ```python
   sc._jsc.hadoopConfiguration().set('mapreduce.input.fileinputformat.input.dir.recursive', 'true')
   ```

    <span data-ttu-id="4048c-148">Esse código configura o Spark para acessar de forma recursiva a estrutura de diretórios para os dados de entrada.</span><span class="sxs-lookup"><span data-stu-id="4048c-148">This code configures Spark to recursively access the directory structure for the input data.</span></span> <span data-ttu-id="4048c-149">A Application Insights Telemetry é registrada em uma estrutura de diretórios semelhante ao `/{telemetry type}/YYYY-MM-DD/{##}/`.</span><span class="sxs-lookup"><span data-stu-id="4048c-149">Application Insights telemetry is logged to a directory structure similar to the `/{telemetry type}/YYYY-MM-DD/{##}/`.</span></span>

4. <span data-ttu-id="4048c-150">Use **SHIFT+ENTER** para executar o código.</span><span class="sxs-lookup"><span data-stu-id="4048c-150">Use **SHIFT+ENTER** to run the code.</span></span> <span data-ttu-id="4048c-151">No lado esquerdo da célula, um '\*' aparece entre colchetes para indicar que o código nessa célula está sendo executado.</span><span class="sxs-lookup"><span data-stu-id="4048c-151">On the left side of the cell, an '\*' appears between the brackets to indicate that the code in this cell is being executed.</span></span> <span data-ttu-id="4048c-152">Quando concluído, o '\*' é alterado para um número e uma saída semelhante ao seguinte é exibida abaixo da célula:</span><span class="sxs-lookup"><span data-stu-id="4048c-152">Once it completes, the '\*' changes to a number, and output similar to the following text is displayed below the cell:</span></span>

        Creating SparkContext as 'sc'

        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    pyspark    idle    Link    Link    ✔

        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. <span data-ttu-id="4048c-153">Uma nova célula é criada abaixo da primeira.</span><span class="sxs-lookup"><span data-stu-id="4048c-153">A new cell is created below the first one.</span></span> <span data-ttu-id="4048c-154">Digite o texto a seguir na nova célula.</span><span class="sxs-lookup"><span data-stu-id="4048c-154">Enter the following text in the new cell.</span></span> <span data-ttu-id="4048c-155">Substitua `CONTAINER` e `STORAGEACCOUNT` pelo nome da conta de armazenamento do Azure e pelo nome do contêiner de blob que contém dados do Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-155">Replace `CONTAINER` and `STORAGEACCOUNT` with the Azure storage account name and blob container name that contains Application Insights data.</span></span>

   ```python
   %%bash
   hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   ```

    <span data-ttu-id="4048c-156">Use **SHIFT+ENTER** para executar essa célula.</span><span class="sxs-lookup"><span data-stu-id="4048c-156">Use **SHIFT+ENTER** to execute this cell.</span></span> <span data-ttu-id="4048c-157">Você verá um resultado semelhante ao seguinte texto:</span><span class="sxs-lookup"><span data-stu-id="4048c-157">You see a result similar to the following text:</span></span>

        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831

    <span data-ttu-id="4048c-158">O caminho wasb retornado é o local dos dados de telemetria do Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-158">The wasb path returned is the location of the Application Insights telemetry data.</span></span> <span data-ttu-id="4048c-159">Altere a linha `hdfs dfs -ls` na célula para usar o caminho wasb retornado, em seguida, use **SHIFT+ENTER** para executar a célula novamente.</span><span class="sxs-lookup"><span data-stu-id="4048c-159">Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again.</span></span> <span data-ttu-id="4048c-160">Desta vez, os resultados devem exibir os diretórios que contêm dados de telemetria.</span><span class="sxs-lookup"><span data-stu-id="4048c-160">This time, the results should display the directories that contain telemetry data.</span></span>

   > [!NOTE]
   > <span data-ttu-id="4048c-161">Para o restante das etapas desta seção, foi usado o diretório `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests`.</span><span class="sxs-lookup"><span data-stu-id="4048c-161">For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used.</span></span> <span data-ttu-id="4048c-162">A estrutura de diretório pode ser diferente.</span><span class="sxs-lookup"><span data-stu-id="4048c-162">Your directory structure may be different.</span></span>

6. <span data-ttu-id="4048c-163">Na próxima célula, digite o seguinte código: substitua `WASB_PATH` pelo caminho da etapa anterior.</span><span class="sxs-lookup"><span data-stu-id="4048c-163">In the next cell, enter the following code: Replace `WASB_PATH` with the path from the previous step.</span></span>

   ```python
   jsonFiles = sc.textFile('WASB_PATH')
   jsonData = sqlContext.read.json(jsonFiles)
   ```

    <span data-ttu-id="4048c-164">Esse código cria um dataframe dos arquivos JSON exportados pelo processo de exportação contínua.</span><span class="sxs-lookup"><span data-stu-id="4048c-164">This code creates a dataframe from the JSON files exported by the continuous export process.</span></span> <span data-ttu-id="4048c-165">Use **SHIFT+ENTER** para executar essa célula.</span><span class="sxs-lookup"><span data-stu-id="4048c-165">Use **SHIFT+ENTER** to run this cell.</span></span>
7. <span data-ttu-id="4048c-166">Na próxima célula, digite e execute o seguinte para exibir o esquema que o Spark criou para os arquivos JSON:</span><span class="sxs-lookup"><span data-stu-id="4048c-166">In the next cell, enter and run the following to view the schema that Spark created for the JSON files:</span></span>

   ```python
   jsonData.printSchema()
   ```

    <span data-ttu-id="4048c-167">O esquema para cada tipo de telemetria é diferente.</span><span class="sxs-lookup"><span data-stu-id="4048c-167">The schema for each type of telemetry is different.</span></span> <span data-ttu-id="4048c-168">O exemplo a seguir é o esquema gerado para solicitações da Web (dados armazenados no subdiretório `Requests`):</span><span class="sxs-lookup"><span data-stu-id="4048c-168">The following example is the schema that is generated for web requests (data stored in the `Requests` subdirectory):</span></span>

        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)
8. <span data-ttu-id="4048c-169">Use o seguinte para registrar o dataframe como uma tabela temporária e executar uma consulta em relação aos dados:</span><span class="sxs-lookup"><span data-stu-id="4048c-169">Use the following to register the dataframe as a temporary table and run a query against the data:</span></span>

   ```python
   jsonData.registerTempTable("requests")
   df = sqlContext.sql("select context.location.city from requests where context.location.city is not null")
   df.show()
   ```

    <span data-ttu-id="4048c-170">Essa consulta retorna as informações de cidade para os primeiros 20 registros em que context.location.city não é nulo.</span><span class="sxs-lookup"><span data-stu-id="4048c-170">This query returns the city information for the top 20 records where context.location.city is not null.</span></span>

   > [!NOTE]
   > <span data-ttu-id="4048c-171">A estrutura de contexto está presente em toda a telemetria registrada pelo Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-171">The context structure is present in all telemetry logged by Application Insights.</span></span> <span data-ttu-id="4048c-172">O elemento de cidade não pode ser preenchido em seus logs.</span><span class="sxs-lookup"><span data-stu-id="4048c-172">The city element may not be populated in your logs.</span></span> <span data-ttu-id="4048c-173">Use o esquema para identificar outros elementos que você possa consultar e que possam conter dados para seus logs.</span><span class="sxs-lookup"><span data-stu-id="4048c-173">Use the schema to identify other elements that you can query that may contain data for your logs.</span></span>

    <span data-ttu-id="4048c-174">Essa consulta retorna informações semelhantes ao seguinte texto:</span><span class="sxs-lookup"><span data-stu-id="4048c-174">This query returns information similar to the following text:</span></span>

        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## <a name="analyze-the-data-scala"></a><span data-ttu-id="4048c-175">Analisar os dados: Scala</span><span class="sxs-lookup"><span data-stu-id="4048c-175">Analyze the data: Scala</span></span>

1. <span data-ttu-id="4048c-176">No [Portal do Azure](https://portal.azure.com), selecione o Spark no cluster HDInsight.</span><span class="sxs-lookup"><span data-stu-id="4048c-176">From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster.</span></span> <span data-ttu-id="4048c-177">Na seção **Links Rápidos**, selecione **Painéis do Cluster**, em seguida, selecione **Notebook Jupyter** na folha Painel do Cluster__.</span><span class="sxs-lookup"><span data-stu-id="4048c-177">From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.</span></span>

    ![Os painéis de cluster](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)
2. <span data-ttu-id="4048c-179">No canto superior direito da página Jupyter, selecione **Novo**, então, **Scala**.</span><span class="sxs-lookup"><span data-stu-id="4048c-179">In the upper right corner of the Jupyter page, select **New**, and then **Scala**.</span></span> <span data-ttu-id="4048c-180">Uma nova guia do navegador contendo um Bloco de anotações do Jupyter com base em Scala é exibida.</span><span class="sxs-lookup"><span data-stu-id="4048c-180">A new browser tab containing a Scala-based Jupyter Notebook appears.</span></span>
3. <span data-ttu-id="4048c-181">No primeiro campo (denominado **célula**) na página, digite o seguinte texto:</span><span class="sxs-lookup"><span data-stu-id="4048c-181">In the first field (called a **cell**) on the page, enter the following text:</span></span>

   ```scala
   sc.hadoopConfiguration.set("mapreduce.input.fileinputformat.input.dir.recursive", "true")
   ```

    <span data-ttu-id="4048c-182">Esse código configura o Spark para acessar de forma recursiva a estrutura de diretórios para os dados de entrada.</span><span class="sxs-lookup"><span data-stu-id="4048c-182">This code configures Spark to recursively access the directory structure for the input data.</span></span> <span data-ttu-id="4048c-183">A Application Insights Telemetry é registrada em uma estrutura de diretórios semelhante a `/{telemetry type}/YYYY-MM-DD/{##}/`.</span><span class="sxs-lookup"><span data-stu-id="4048c-183">Application Insights telemetry is logged to a directory structure similar to `/{telemetry type}/YYYY-MM-DD/{##}/`.</span></span>

4. <span data-ttu-id="4048c-184">Use **SHIFT+ENTER** para executar o código.</span><span class="sxs-lookup"><span data-stu-id="4048c-184">Use **SHIFT+ENTER** to run the code.</span></span> <span data-ttu-id="4048c-185">No lado esquerdo da célula, um '\*' aparece entre colchetes para indicar que o código nessa célula está sendo executado.</span><span class="sxs-lookup"><span data-stu-id="4048c-185">On the left side of the cell, an '\*' appears between the brackets to indicate that the code in this cell is being executed.</span></span> <span data-ttu-id="4048c-186">Quando concluído, o '\*' é alterado para um número e uma saída semelhante ao seguinte é exibida abaixo da célula:</span><span class="sxs-lookup"><span data-stu-id="4048c-186">Once it completes, the '\*' changes to a number, and output similar to the following text is displayed below the cell:</span></span>

        Creating SparkContext as 'sc'

        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    spark    idle    Link    Link    ✔

        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. <span data-ttu-id="4048c-187">Uma nova célula é criada abaixo da primeira.</span><span class="sxs-lookup"><span data-stu-id="4048c-187">A new cell is created below the first one.</span></span> <span data-ttu-id="4048c-188">Digite o texto a seguir na nova célula.</span><span class="sxs-lookup"><span data-stu-id="4048c-188">Enter the following text in the new cell.</span></span> <span data-ttu-id="4048c-189">Substitua `CONTAINER` e `STORAGEACCOUNT` pelo nome da conta de armazenamento do Azure e pelo nome do contêiner de blob que contém logs do Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-189">Replace `CONTAINER` and `STORAGEACCOUNT` with the Azure storage account name and blob container name that contains Application Insights logs.</span></span>

   ```scala
   %%bash
   hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   ```

    <span data-ttu-id="4048c-190">Use **SHIFT+ENTER** para executar essa célula.</span><span class="sxs-lookup"><span data-stu-id="4048c-190">Use **SHIFT+ENTER** to execute this cell.</span></span> <span data-ttu-id="4048c-191">Você verá um resultado semelhante ao seguinte texto:</span><span class="sxs-lookup"><span data-stu-id="4048c-191">You see a result similar to the following text:</span></span>

        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831

    <span data-ttu-id="4048c-192">O caminho wasb retornado é o local dos dados de telemetria do Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-192">The wasb path returned is the location of the Application Insights telemetry data.</span></span> <span data-ttu-id="4048c-193">Altere a linha `hdfs dfs -ls` na célula para usar o caminho wasb retornado, em seguida, use **SHIFT+ENTER** para executar a célula novamente.</span><span class="sxs-lookup"><span data-stu-id="4048c-193">Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again.</span></span> <span data-ttu-id="4048c-194">Desta vez, os resultados devem exibir os diretórios que contêm dados de telemetria.</span><span class="sxs-lookup"><span data-stu-id="4048c-194">This time, the results should display the directories that contain telemetry data.</span></span>

   > [!NOTE]
   > <span data-ttu-id="4048c-195">Para o restante das etapas desta seção, foi usado o diretório `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests`.</span><span class="sxs-lookup"><span data-stu-id="4048c-195">For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used.</span></span> <span data-ttu-id="4048c-196">Este diretório pode não existir, a menos que os dados de telemetria sejam para um aplicativo Web.</span><span class="sxs-lookup"><span data-stu-id="4048c-196">This directory may not exist unless your telemetry data is for a web app.</span></span>

6. <span data-ttu-id="4048c-197">Na próxima célula, digite o seguinte código: substitua `WASB\_PATH` pelo caminho da etapa anterior.</span><span class="sxs-lookup"><span data-stu-id="4048c-197">In the next cell, enter the following code: Replace `WASB\_PATH` with the path from the previous step.</span></span>

   ```scala
   var jsonFiles = sc.textFile('WASB_PATH')
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   var jsonData = sqlContext.read.json(jsonFiles)
   ```

    <span data-ttu-id="4048c-198">Esse código cria um dataframe dos arquivos JSON exportados pelo processo de exportação contínua.</span><span class="sxs-lookup"><span data-stu-id="4048c-198">This code creates a dataframe from the JSON files exported by the continuous export process.</span></span> <span data-ttu-id="4048c-199">Use **SHIFT+ENTER** para executar essa célula.</span><span class="sxs-lookup"><span data-stu-id="4048c-199">Use **SHIFT+ENTER** to run this cell.</span></span>

7. <span data-ttu-id="4048c-200">Na próxima célula, digite e execute o seguinte para exibir o esquema que o Spark criou para os arquivos JSON:</span><span class="sxs-lookup"><span data-stu-id="4048c-200">In the next cell, enter and run the following to view the schema that Spark created for the JSON files:</span></span>

   ```scala
   jsonData.printSchema
   ```

    <span data-ttu-id="4048c-201">O esquema para cada tipo de telemetria é diferente.</span><span class="sxs-lookup"><span data-stu-id="4048c-201">The schema for each type of telemetry is different.</span></span> <span data-ttu-id="4048c-202">O exemplo a seguir é o esquema gerado para solicitações da Web (dados armazenados no subdiretório `Requests`):</span><span class="sxs-lookup"><span data-stu-id="4048c-202">The following example is the schema that is generated for web requests (data stored in the `Requests` subdirectory):</span></span>

        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)

8. <span data-ttu-id="4048c-203">Use o seguinte para registrar o dataframe como uma tabela temporária e executar uma consulta em relação aos dados:</span><span class="sxs-lookup"><span data-stu-id="4048c-203">Use the following to register the dataframe as a temporary table and run a query against the data:</span></span>

   ```scala
   jsonData.registerTempTable("requests")
   var city = sqlContext.sql("select context.location.city from requests where context.location.city is not null limit 10").show()
   ```

    <span data-ttu-id="4048c-204">Essa consulta retorna as informações de cidade para os primeiros 20 registros em que context.location.city não é nulo.</span><span class="sxs-lookup"><span data-stu-id="4048c-204">This query returns the city information for the top 20 records where context.location.city is not null.</span></span>

   > [!NOTE]
   > <span data-ttu-id="4048c-205">A estrutura de contexto está presente em toda a telemetria registrada pelo Application Insights.</span><span class="sxs-lookup"><span data-stu-id="4048c-205">The context structure is present in all telemetry logged by Application Insights.</span></span> <span data-ttu-id="4048c-206">O elemento de cidade não pode ser preenchido em seus logs.</span><span class="sxs-lookup"><span data-stu-id="4048c-206">The city element may not be populated in your logs.</span></span> <span data-ttu-id="4048c-207">Use o esquema para identificar outros elementos que você possa consultar e que possam conter dados para seus logs.</span><span class="sxs-lookup"><span data-stu-id="4048c-207">Use the schema to identify other elements that you can query that may contain data for your logs.</span></span>
   >
   >

    <span data-ttu-id="4048c-208">Essa consulta retorna informações semelhantes ao seguinte texto:</span><span class="sxs-lookup"><span data-stu-id="4048c-208">This query returns information similar to the following text:</span></span>

        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## <a name="next-steps"></a><span data-ttu-id="4048c-209">Próximas etapas</span><span class="sxs-lookup"><span data-stu-id="4048c-209">Next steps</span></span>

<span data-ttu-id="4048c-210">Para obter mais exemplos de uso do Spark para trabalhar com dados e serviços no Azure, veja os seguintes documentos:</span><span class="sxs-lookup"><span data-stu-id="4048c-210">For more examples of using Spark to work with data and services in Azure, see the following documents:</span></span>

* [<span data-ttu-id="4048c-211">Spark com BI: executar análise de dados interativa usando o Spark no HDInsight com ferramentas de BI</span><span class="sxs-lookup"><span data-stu-id="4048c-211">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="4048c-212">Spark com Aprendizado de Máquina: usar o Spark no HDInsight para analisar a temperatura de prédios usando dados do sistema HVAC</span><span class="sxs-lookup"><span data-stu-id="4048c-212">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* <span data-ttu-id="4048c-213">
            [Spark com Machine Learning: usar o Spark no HDInsight para prever resultados da inspeção de alimentos](hdinsight-apache-spark-machine-learning-mllib-ipython.md)</span><span class="sxs-lookup"><span data-stu-id="4048c-213">[Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results](hdinsight-apache-spark-machine-learning-mllib-ipython.md)</span></span>
* [<span data-ttu-id="4048c-214">Streaming Spark: usar o Spark no HDInsight para a criação de aplicativos de streaming</span><span class="sxs-lookup"><span data-stu-id="4048c-214">Spark Streaming: Use Spark in HDInsight for building streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="4048c-215">Análise de log do site usando o Spark no HDInsight</span><span class="sxs-lookup"><span data-stu-id="4048c-215">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

<span data-ttu-id="4048c-216">Para saber mais sobre a criação e a execução de aplicativos Spark, veja os seguintes documentos:</span><span class="sxs-lookup"><span data-stu-id="4048c-216">For information on creating and running Spark applications, see the following documents:</span></span>

* [<span data-ttu-id="4048c-217">Criar um aplicativo autônomo usando Scala</span><span class="sxs-lookup"><span data-stu-id="4048c-217">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="4048c-218">Executar trabalhos remotamente em um cluster do Spark usando Livy</span><span class="sxs-lookup"><span data-stu-id="4048c-218">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)
