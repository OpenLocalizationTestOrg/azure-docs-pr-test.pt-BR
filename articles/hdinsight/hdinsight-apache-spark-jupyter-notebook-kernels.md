---
title: "Kernels para o bloco de anotações do Jupyter em clusters do Spark no Azure HDInsight | Microsoft Docs"
description: "Saiba mais sobre os kernels PySpark, PySpark3 e Spark para o notebook do Jupyter disponíveis com clusters do Spark no Azure HDInsight."
keywords: "bloco de anotações do jupyter no spark, jupyter spark"
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 6cfd1c1e7b22f5460b78687c815d149e6c6deac9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: pt-BR
ms.lasthandoff: 07/11/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="09282-104">Kernels para o bloco de anotações do Jupyter em clusters do Spark no Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="09282-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="09282-105">Os clusters do HDInsight Spark fornecem kernels que você pode usar com o bloco de anotações do Jupyter no Spark para testar seus aplicativos.</span><span class="sxs-lookup"><span data-stu-id="09282-105">HDInsight Spark clusters provide kernels that you can use with the Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="09282-106">Um kernel é um programa que é executado e que interpreta seu código.</span><span class="sxs-lookup"><span data-stu-id="09282-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="09282-107">Os três kernels são:</span><span class="sxs-lookup"><span data-stu-id="09282-107">The three kernels are:</span></span>

- <span data-ttu-id="09282-108">**PySpark** - para aplicativos escritos em Python2</span><span class="sxs-lookup"><span data-stu-id="09282-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="09282-109">**PySpark3** - para aplicativos escritos em Python3</span><span class="sxs-lookup"><span data-stu-id="09282-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="09282-110">**Spark** - para aplicativos escritos em Scala</span><span class="sxs-lookup"><span data-stu-id="09282-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="09282-111">Neste artigo, você aprenderá como usar esses kernels e os benefícios de usá-los.</span><span class="sxs-lookup"><span data-stu-id="09282-111">In this article, you learn how to use these kernels and the benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="09282-112">Pré-requisitos</span><span class="sxs-lookup"><span data-stu-id="09282-112">Prerequisites</span></span>

* <span data-ttu-id="09282-113">Um cluster do Apache Spark no HDInsight.</span><span class="sxs-lookup"><span data-stu-id="09282-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="09282-114">Para obter instruções, consulte o artigo sobre como [Criar clusters do Apache Spark no Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="09282-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="09282-115">Criar um bloco de anotações do Jupyter no Spark HDInsight</span><span class="sxs-lookup"><span data-stu-id="09282-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="09282-116">Do [portal do Azure](https://portal.azure.com/), abra o seu cluster.</span><span class="sxs-lookup"><span data-stu-id="09282-116">From the [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="09282-117">Consulte [lista e mostrar clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) para obter instruções.</span><span class="sxs-lookup"><span data-stu-id="09282-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for the instructions.</span></span> <span data-ttu-id="09282-118">O cluster é aberto em uma nova folha de portal.</span><span class="sxs-lookup"><span data-stu-id="09282-118">The cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="09282-119">Do **links rápidos** seção, clique em **Painéis do cluster** para abrir a folha **Painéis do cluster** folha.</span><span class="sxs-lookup"><span data-stu-id="09282-119">From the **Quick links** section, click **Cluster dashboards** to open the **Cluster dashboards** blade.</span></span>  <span data-ttu-id="09282-120">Se você não vir **Links rápidos**, clique em **visão geral** no menu à esquerda na folha.</span><span class="sxs-lookup"><span data-stu-id="09282-120">If you don't see **Quick Links**, click **Overview** from the left menu on the blade.</span></span>

    <span data-ttu-id="09282-121">![Bloco de anotações do Jupyter no Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Bloco de anotações do Jupyter no Spark")</span><span class="sxs-lookup"><span data-stu-id="09282-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="09282-122">Clique em **Notebook Jupyter**.</span><span class="sxs-lookup"><span data-stu-id="09282-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="09282-123">Se você receber uma solicitação, insira as credenciais de administrador para o cluster.</span><span class="sxs-lookup"><span data-stu-id="09282-123">If prompted, enter the admin credentials for the cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="09282-124">Você também pode acessar o bloco de anotações do Jupyter de seu cluster do Spark abrindo o seguinte URL no navegador.</span><span class="sxs-lookup"><span data-stu-id="09282-124">You may also reach the Jupyter notebook on Spark cluster by opening the following URL in your browser.</span></span> <span data-ttu-id="09282-125">Substitua **CLUSTERNAME** pelo nome do cluster:</span><span class="sxs-lookup"><span data-stu-id="09282-125">Replace **CLUSTERNAME** with the name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="09282-126">Clique em **novo**e, em seguida, clique em **Pyspark**, **PySpark3** ou **Spark** para criar um notebook.</span><span class="sxs-lookup"><span data-stu-id="09282-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** to create a notebook.</span></span> <span data-ttu-id="09282-127">Use o kernel Spark para aplicativos em Scala, o kernel PySpark para aplicativos em Python2 e o kernel PySpark3 para aplicativos em Python3.</span><span class="sxs-lookup"><span data-stu-id="09282-127">Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="09282-128">![Kernels para bloco de anotações do Jupyter no Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels para bloco de anotações do Jupyter no Spark")</span><span class="sxs-lookup"><span data-stu-id="09282-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="09282-129">Um notebook é aberto com o kernel selecionado.</span><span class="sxs-lookup"><span data-stu-id="09282-129">A notebook opens with the kernel you selected.</span></span>

## <a name="benefits-of-using-the-kernels"></a><span data-ttu-id="09282-130">Benefícios de usar os kernels</span><span class="sxs-lookup"><span data-stu-id="09282-130">Benefits of using the kernels</span></span>

<span data-ttu-id="09282-131">Estes são alguns dos benefícios de usar os novos kernels com o bloco de anotações do Jupyter nos clusters do Spark HDInsight.</span><span class="sxs-lookup"><span data-stu-id="09282-131">Here are a few benefits of using the new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="09282-132">**Contextos de predefinição**.</span><span class="sxs-lookup"><span data-stu-id="09282-132">**Preset contexts**.</span></span> <span data-ttu-id="09282-133">Com os kernels **PySpark**, **PySpark3** ou **Spark**, não é necessário definir os contextos Spark ou Hive explicitamente antes de começar a trabalhar com seus aplicativos.</span><span class="sxs-lookup"><span data-stu-id="09282-133">With  **PySpark**, **PySpark3**, or the **Spark** kernels, you do not need to set the Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="09282-134">Eles estão disponíveis para você por padrão.</span><span class="sxs-lookup"><span data-stu-id="09282-134">These are available by default.</span></span> <span data-ttu-id="09282-135">Esses contextos são:</span><span class="sxs-lookup"><span data-stu-id="09282-135">These contexts are:</span></span>
   
   * <span data-ttu-id="09282-136">**sc** - para o contexto do Spark</span><span class="sxs-lookup"><span data-stu-id="09282-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="09282-137">**sqlContext** : para o contexto Hive</span><span class="sxs-lookup"><span data-stu-id="09282-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="09282-138">Portanto, você não precisa executar instruções como as seguintes para definir os contextos:</span><span class="sxs-lookup"><span data-stu-id="09282-138">So, you don't have to run statements like the following to set the contexts:</span></span>

        <span data-ttu-id="09282-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="09282-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="09282-140">Em vez disso, pode usar os contextos predefinidos diretamente em seu aplicativo.</span><span class="sxs-lookup"><span data-stu-id="09282-140">Instead, you can directly use the preset contexts in your application.</span></span>

- <span data-ttu-id="09282-141">**A mágica da célula**.</span><span class="sxs-lookup"><span data-stu-id="09282-141">**Cell magics**.</span></span> <span data-ttu-id="09282-142">O kernel PySpark fornece algumas “mágicas” predefinidas, que são comandos especiais que podem ser chamados com `%%` (por exemplo, `%%MAGIC` <args>).</span><span class="sxs-lookup"><span data-stu-id="09282-142">The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="09282-143">O comando mágico deve ser a primeira palavra em uma célula do código e de permitir várias linhas de conteúdo.</span><span class="sxs-lookup"><span data-stu-id="09282-143">The magic command must be the first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="09282-144">A palavra mágica deve ser a primeira palavra na célula.</span><span class="sxs-lookup"><span data-stu-id="09282-144">The magic word should be the first word in the cell.</span></span> <span data-ttu-id="09282-145">Adicionar algo antes da palavra mágica, até mesmo comentários, causa um erro.</span><span class="sxs-lookup"><span data-stu-id="09282-145">Adding anything before the magic, even comments, causes an error.</span></span>     <span data-ttu-id="09282-146">Para saber mais sobre palavras mágicas, clique [aqui](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span><span class="sxs-lookup"><span data-stu-id="09282-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="09282-147">A tabela a seguir lista as diferentes palavras mágicas disponíveis por meio dos kernels.</span><span class="sxs-lookup"><span data-stu-id="09282-147">The following table lists the different magics available through the kernels.</span></span>

   | <span data-ttu-id="09282-148">Mágica</span><span class="sxs-lookup"><span data-stu-id="09282-148">Magic</span></span> | <span data-ttu-id="09282-149">Exemplo</span><span class="sxs-lookup"><span data-stu-id="09282-149">Example</span></span> | <span data-ttu-id="09282-150">Descrição</span><span class="sxs-lookup"><span data-stu-id="09282-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="09282-151">ajuda</span><span class="sxs-lookup"><span data-stu-id="09282-151">help</span></span> |`%%help` |<span data-ttu-id="09282-152">Gera uma tabela de todos os comandos mágicos disponíveis com exemplo e descrição</span><span class="sxs-lookup"><span data-stu-id="09282-152">Generates a table of all the available magics with example and description</span></span> |
   | <span data-ttu-id="09282-153">informações</span><span class="sxs-lookup"><span data-stu-id="09282-153">info</span></span> |`%%info` |<span data-ttu-id="09282-154">Envia informações de sessão para o ponto de extremidade Livy atual</span><span class="sxs-lookup"><span data-stu-id="09282-154">Outputs session information for the current Livy endpoint</span></span> |
   | <span data-ttu-id="09282-155">CONFIGURAR</span><span class="sxs-lookup"><span data-stu-id="09282-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="09282-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="09282-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="09282-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="09282-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="09282-158">Configura os parâmetros para a criação de uma sessão.</span><span class="sxs-lookup"><span data-stu-id="09282-158">Configures the parameters for creating a session.</span></span> <span data-ttu-id="09282-159">O sinalizador force (-f) será obrigatório se uma sessão já tiver sido criada, o que garante que a sessão será descartada e recriada.</span><span class="sxs-lookup"><span data-stu-id="09282-159">The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated.</span></span> <span data-ttu-id="09282-160">Veja o [Corpo da Solicitação POST /sessions da Livy](https://github.com/cloudera/livy#request-body) para obter uma lista de parâmetros válidos.</span><span class="sxs-lookup"><span data-stu-id="09282-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="09282-161">Os parâmetros devem ser passados como uma cadeia de caracteres JSON e devem estar na linha seguinte, logo após a mágica, conforme mostrado na coluna de exemplo.</span><span class="sxs-lookup"><span data-stu-id="09282-161">Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span> |
   | <span data-ttu-id="09282-162">sql</span><span class="sxs-lookup"><span data-stu-id="09282-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="09282-163">Executa uma consulta do Hive no dqlContext.</span><span class="sxs-lookup"><span data-stu-id="09282-163">Executes a Hive query against the sqlContext.</span></span> <span data-ttu-id="09282-164">Se o parâmetro `-o` for passado, o resultado da consulta será persistido no contexto %%local do Python como um dataframe do [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="09282-164">If the `-o` parameter is passed, the result of the query is persisted in the %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="09282-165">local</span><span class="sxs-lookup"><span data-stu-id="09282-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="09282-166">Todo o código nas linhas subsequentes é executado localmente.</span><span class="sxs-lookup"><span data-stu-id="09282-166">All the code in subsequent lines is executed locally.</span></span> <span data-ttu-id="09282-167">O código deve ser um código Python2 válido, independentemente do kernel que você está usando.</span><span class="sxs-lookup"><span data-stu-id="09282-167">Code must be valid Python2 code even irrespective of the kernel you are using.</span></span> <span data-ttu-id="09282-168">Portanto, mesmo se você selecionou os kernels **PySpark3** ou **Spark** ao criar o notebook, se você usar a palavra mágica `%%local` em uma célula, essa célula só poderá ter um código Python2 válido...</span><span class="sxs-lookup"><span data-stu-id="09282-168">So, even if you selected **PySpark3** or **Spark** kernels while creating the notebook, if you use the `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="09282-169">logs</span><span class="sxs-lookup"><span data-stu-id="09282-169">logs</span></span> |`%%logs` |<span data-ttu-id="09282-170">Gera os logs da sessão atual do Livy.</span><span class="sxs-lookup"><span data-stu-id="09282-170">Outputs the logs for the current Livy session.</span></span> |
   | <span data-ttu-id="09282-171">excluir</span><span class="sxs-lookup"><span data-stu-id="09282-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="09282-172">Exclui uma sessão específica do ponto de extremidade atual do Livy.</span><span class="sxs-lookup"><span data-stu-id="09282-172">Deletes a specific session of the current Livy endpoint.</span></span> <span data-ttu-id="09282-173">Observe que você não pode excluir a sessão iniciada para o próprio kernel.</span><span class="sxs-lookup"><span data-stu-id="09282-173">Note that you cannot delete the session that is initiated for the kernel itself.</span></span> |
   | <span data-ttu-id="09282-174">limpeza</span><span class="sxs-lookup"><span data-stu-id="09282-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="09282-175">Exclui todas as sessões do ponto de extremidade atual do Livy, incluindo a sessão deste notebook.</span><span class="sxs-lookup"><span data-stu-id="09282-175">Deletes all the sessions for the current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="09282-176">O sinalizador de força -f é obrigatório.</span><span class="sxs-lookup"><span data-stu-id="09282-176">The force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="09282-177">Além das mágicas adicionados pelo kernel PySpark, você também pode usar as [mágicas internas do IPython](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), incluindo `%%sh`.</span><span class="sxs-lookup"><span data-stu-id="09282-177">In addition to the magics added by the PySpark kernel, you can also use the [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="09282-178">Você pode usar a mágica `%%sh` para executar scripts e bloco de código no nó principal do cluster.</span><span class="sxs-lookup"><span data-stu-id="09282-178">You can use the `%%sh` magic to run scripts and block of code on the cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="09282-179">**Visualização automática**.</span><span class="sxs-lookup"><span data-stu-id="09282-179">**Auto visualization**.</span></span> <span data-ttu-id="09282-180">O kernel **Pyspark** visualiza automaticamente a saída das consultas Hive e SQL.</span><span class="sxs-lookup"><span data-stu-id="09282-180">The **Pyspark** kernel automatically visualizes the output of Hive and SQL queries.</span></span> <span data-ttu-id="09282-181">Escolha entre vários tipos diferentes de visualização, incluindo Tabela, Pizza, Linha, Área, Barra.</span><span class="sxs-lookup"><span data-stu-id="09282-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-the-sql-magic"></a><span data-ttu-id="09282-182">Parâmetros compatíveis com a mágica de %%sql</span><span class="sxs-lookup"><span data-stu-id="09282-182">Parameters supported with the %%sql magic</span></span>
<span data-ttu-id="09282-183">A palavra mágica `%%sql` é compatível com diversos parâmetros que podem ser usados para controlar o tipo de saída que você recebe ao executar consultas.</span><span class="sxs-lookup"><span data-stu-id="09282-183">The `%%sql` magic supports different parameters that you can use to control the kind of output that you receive when you run queries.</span></span> <span data-ttu-id="09282-184">A tabela a seguir lista as saídas.</span><span class="sxs-lookup"><span data-stu-id="09282-184">The following table lists the output.</span></span>

| <span data-ttu-id="09282-185">Parâmetro</span><span class="sxs-lookup"><span data-stu-id="09282-185">Parameter</span></span> | <span data-ttu-id="09282-186">Exemplo</span><span class="sxs-lookup"><span data-stu-id="09282-186">Example</span></span> | <span data-ttu-id="09282-187">Descrição</span><span class="sxs-lookup"><span data-stu-id="09282-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="09282-188">-o</span><span class="sxs-lookup"><span data-stu-id="09282-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="09282-189">Use esse parâmetro para manter o resultado da consulta, no contexto Python %%local, como um dataframe [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="09282-189">Use this parameter to persist the result of the query, in the %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="09282-190">O nome da variável dataframe é o nome da variável que você especificar.</span><span class="sxs-lookup"><span data-stu-id="09282-190">The name of the dataframe variable is the variable name you specify.</span></span> |
| <span data-ttu-id="09282-191">-q</span><span class="sxs-lookup"><span data-stu-id="09282-191">-q</span></span> |`-q` |<span data-ttu-id="09282-192">Use esta opção para desativar visualizações da célula.</span><span class="sxs-lookup"><span data-stu-id="09282-192">Use this to turn off visualizations for the cell.</span></span> <span data-ttu-id="09282-193">Se não desejar visualizar o conteúdo de uma célula automaticamente, mas apenas capturá-la como um dataframe, use `-q -o <VARIABLE>`.</span><span class="sxs-lookup"><span data-stu-id="09282-193">If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="09282-194">Se desejar desativar as visualizações sem capturar os resultados (por exemplo, para executar uma consulta SQL, como uma instrução `CREATE TABLE`), use `-q` sem especificar um argumento `-o`.</span><span class="sxs-lookup"><span data-stu-id="09282-194">If you want to turn off visualizations without capturing the results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="09282-195">-m</span><span class="sxs-lookup"><span data-stu-id="09282-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="09282-196">Onde **METHOD** é **take** ou **sample** (o padrão é **take**).</span><span class="sxs-lookup"><span data-stu-id="09282-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="09282-197">Se o método for **take**, o kernel selecionará elementos da parte superior do conjunto de dados de resultados especificado por MAXROWS (descrito posteriormente nesta tabela).</span><span class="sxs-lookup"><span data-stu-id="09282-197">If the method is **take**, the kernel picks elements from the top of the result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="09282-198">Se o método for **sample**, o kernel experimentará aleatoriamente os elementos do conjunto de dados segundo o parâmetro `-r`, descrito a seguir nesta tabela.</span><span class="sxs-lookup"><span data-stu-id="09282-198">If the method is **sample**, the kernel randomly samples elements of the data set according to `-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="09282-199">-r</span><span class="sxs-lookup"><span data-stu-id="09282-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="09282-200">Aqui **FRACTION** é um número de ponto flutuante entre 0.0 e 1.0.</span><span class="sxs-lookup"><span data-stu-id="09282-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="09282-201">Se o método de amostragem para a consulta SQL for `sample`, o kernel experimentará aleatoriamente a fração especificada dos elementos do conjunto de resultados.</span><span class="sxs-lookup"><span data-stu-id="09282-201">If the sample method for the SQL query is `sample`, then the kernel randomly samples the specified fraction of the elements of the result set for you.</span></span> <span data-ttu-id="09282-202">Por exemplo, se você executar uma consulta SQL com os argumentos `-m sample -r 0.01`, 1% das linhas resultantes serão amostradas aleatoriamente.</span><span class="sxs-lookup"><span data-stu-id="09282-202">For example, if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="09282-203">**MAXROWS** é um valor inteiro.</span><span class="sxs-lookup"><span data-stu-id="09282-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="09282-204">O kernel limita o número de linhas de saída para **MAXROWS**.</span><span class="sxs-lookup"><span data-stu-id="09282-204">The kernel limits the number of output rows to **MAXROWS**.</span></span> <span data-ttu-id="09282-205">Se **MAXROWS** for um número negativo como **-1**, o número de linhas no conjunto de resultados não será limitado.</span><span class="sxs-lookup"><span data-stu-id="09282-205">If **MAXROWS** is a negative number such as **-1**, then the number of rows in the result set is not limited.</span></span> |

<span data-ttu-id="09282-206">**Exemplo:**</span><span class="sxs-lookup"><span data-stu-id="09282-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="09282-207">A instrução acima faz o seguinte:</span><span class="sxs-lookup"><span data-stu-id="09282-207">The statement above does the following:</span></span>

* <span data-ttu-id="09282-208">Seleciona todos os registros de **hivesampletable**.</span><span class="sxs-lookup"><span data-stu-id="09282-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="09282-209">Como usamos - q, ele desativa a visualização automática.</span><span class="sxs-lookup"><span data-stu-id="09282-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="09282-210">Como usamos `-m sample -r 0.1 -n 500` , ele recolhe um exemplo de 10% das linhas na hivesampletable aleatoriamente e limita o tamanho do conjunto de resultados a 500 linhas.</span><span class="sxs-lookup"><span data-stu-id="09282-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.</span></span>
* <span data-ttu-id="09282-211">Por fim, como usamos `-o query2` , ele também salva a saída em um dataframe chamado **query2**.</span><span class="sxs-lookup"><span data-stu-id="09282-211">Finally, because we used `-o query2` it also saves the output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-the-new-kernels"></a><span data-ttu-id="09282-212">Considerações ao usar os novos kernels</span><span class="sxs-lookup"><span data-stu-id="09282-212">Considerations while using the new kernels</span></span>

<span data-ttu-id="09282-213">Seja qual for o kernel usado, deixar os notebooks em execução consumirá os recursos de cluster.</span><span class="sxs-lookup"><span data-stu-id="09282-213">Whichever kernel you use, leaving the notebooks running consumes the cluster resources.</span></span>  <span data-ttu-id="09282-214">Com esses kernels, como os contextos são predefinidos, simplesmente sair dos notebooks não elimina o contexto e, portanto, os recursos do cluster continuam em uso.</span><span class="sxs-lookup"><span data-stu-id="09282-214">With these kernels, because the contexts are preset, simply exiting the notebooks does not kill the context and hence the cluster resources continue to be in use.</span></span> <span data-ttu-id="09282-215">Uma prática recomendada é usar a opção **Fechar e Interromper** do menu **Arquivo** do notebook quando você terminar de usar o notebook, o que elimina o contexto e sai do notebook.</span><span class="sxs-lookup"><span data-stu-id="09282-215">A good practice is to use the **Close and Halt** option from the notebook's **File** menu when you are finished using the notebook, which kills the context and then exits the notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="09282-216">Mostre-me alguns exemplos</span><span class="sxs-lookup"><span data-stu-id="09282-216">Show me some examples</span></span>

<span data-ttu-id="09282-217">Quando você abrir um notebook Jupyter, verá duas pastas disponíveis no nível raiz.</span><span class="sxs-lookup"><span data-stu-id="09282-217">When you open a Jupyter notebook, you see two folders available at the root level.</span></span>

* <span data-ttu-id="09282-218">A pasta **PySpark** tem notebooks de amostra que usam o novo kernel do **Python**.</span><span class="sxs-lookup"><span data-stu-id="09282-218">The **PySpark** folder has sample notebooks that use the new **Python** kernel.</span></span>
* <span data-ttu-id="09282-219">A pasta **Scala** tem notebooks de amostra que usam o novo kernel do **Spark**.</span><span class="sxs-lookup"><span data-stu-id="09282-219">The **Scala** folder has sample notebooks that use the new **Spark** kernel.</span></span>

<span data-ttu-id="09282-220">Você pode abrir o notebook **00 - [READ ME FIRST] Spark Magic Kernel Features** na pasta **PySpark** ou **Spark** para saber mais sobre as diferentes mágicas disponíveis.</span><span class="sxs-lookup"><span data-stu-id="09282-220">You can open the **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from the **PySpark** or **Spark** folder to learn about the different magics available.</span></span> <span data-ttu-id="09282-221">Você também pode usar outros notebooks de exemplo disponíveis nas duas pastas para saber como obter diferentes cenários usando notebooks Jupyter com clusters Spark HDInsight.</span><span class="sxs-lookup"><span data-stu-id="09282-221">You can also use the other sample notebooks available under the two folders to learn how to achieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-the-notebooks-stored"></a><span data-ttu-id="09282-222">Onde os blocos de anotações são armazenados?</span><span class="sxs-lookup"><span data-stu-id="09282-222">Where are the notebooks stored?</span></span>

<span data-ttu-id="09282-223">Os notebooks Jupyter são salvos na conta de armazenamento associada ao cluster na pasta **/HdiNotebooks** .</span><span class="sxs-lookup"><span data-stu-id="09282-223">Jupyter notebooks are saved to the storage account associated with the cluster under the **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="09282-224">Os notebooks, arquivos de texto e pastas que você cria no Jupyter podem ser acessados na conta de armazenamento.</span><span class="sxs-lookup"><span data-stu-id="09282-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.</span></span>  <span data-ttu-id="09282-225">Por exemplo, se você usar o Jupyter para criar uma pasta **myfolder** e um notebook **myfolder/mynotebook.ipynb**, poderá acessar esse notebook em `/HdiNotebooks/myfolder/mynotebook.ipynb` dentro da conta de armazenamento.</span><span class="sxs-lookup"><span data-stu-id="09282-225">For example, if you use Jupyter to create a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within the storage account.</span></span>  <span data-ttu-id="09282-226">O inverso também é possível, ou seja, se você carregar um notebook diretamente em sua conta de armazenamento em `/HdiNotebooks/mynotebook1.ipynb`, ele também ficará visível no Jupyter.</span><span class="sxs-lookup"><span data-stu-id="09282-226">The reverse is also true, that is, if you upload a notebook directly to your storage account at `/HdiNotebooks/mynotebook1.ipynb`, the notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="09282-227">Os logs são mantidos na conta de armazenamento mesmo após a exclusão do cluster.</span><span class="sxs-lookup"><span data-stu-id="09282-227">Notebooks remain in the storage account even after the cluster is deleted.</span></span>

<span data-ttu-id="09282-228">A forma como os blocos de anotações são salvos na conta de armazenamento é compatível com HDFS.</span><span class="sxs-lookup"><span data-stu-id="09282-228">The way notebooks are saved to the storage account is compatible with HDFS.</span></span> <span data-ttu-id="09282-229">Portanto, se você se conectar por SSH ao cluster, poderá usar comandos de gerenciamento de arquivos, como mostra o trecho a seguir:</span><span class="sxs-lookup"><span data-stu-id="09282-229">So, if you SSH into the cluster you can use file management commands as shown in the following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


<span data-ttu-id="09282-230">Caso haja problemas para acessar a conta de armazenamento do cluster, os notebooks também são salvos no `/var/lib/jupyter`do nó principal.</span><span class="sxs-lookup"><span data-stu-id="09282-230">In case there are issues accessing the storage account for the cluster, the notebooks are also saved on the headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="09282-231">Navegador com suporte</span><span class="sxs-lookup"><span data-stu-id="09282-231">Supported browser</span></span>

<span data-ttu-id="09282-232">Os blocos de anotações do Jupyter em clusters do Spark HDInsight só têm suporte no Google Chrome.</span><span class="sxs-lookup"><span data-stu-id="09282-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="09282-233">Comentários</span><span class="sxs-lookup"><span data-stu-id="09282-233">Feedback</span></span>
<span data-ttu-id="09282-234">Os kernels novos estão evoluindo e amadurecerão com o tempo.</span><span class="sxs-lookup"><span data-stu-id="09282-234">The new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="09282-235">Isso também pode significar que as APIs podem mudar à medida que esses kernels amadurecem.</span><span class="sxs-lookup"><span data-stu-id="09282-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="09282-236">Agradecemos o envio quaisquer comentários que você tenha ao usar esses novos kernels.</span><span class="sxs-lookup"><span data-stu-id="09282-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="09282-237">Isso é muito útil na formação da versão final desses kernels.</span><span class="sxs-lookup"><span data-stu-id="09282-237">This is useful in shaping the final release of these kernels.</span></span> <span data-ttu-id="09282-238">Você pode deixar seus comentários/feedback na seção **Comentários** no final deste artigo.</span><span class="sxs-lookup"><span data-stu-id="09282-238">You can leave your comments/feedback under the **Comments** section at the bottom of this article.</span></span>

## <span data-ttu-id="09282-239"><a name="seealso"></a>Consulte também</span><span class="sxs-lookup"><span data-stu-id="09282-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="09282-240">Visão geral: Apache Spark no Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="09282-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="09282-241">Cenários</span><span class="sxs-lookup"><span data-stu-id="09282-241">Scenarios</span></span>
* [<span data-ttu-id="09282-242">Spark com BI: executar análise de dados interativa usando o Spark no HDInsight com ferramentas de BI</span><span class="sxs-lookup"><span data-stu-id="09282-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="09282-243">Spark com Aprendizado de Máquina: usar o Spark no HDInsight para analisar a temperatura de prédios usando dados do sistema HVAC</span><span class="sxs-lookup"><span data-stu-id="09282-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="09282-244">Spark com Aprendizado de Máquina: usar o Spark no HDInsight para prever resultados da inspeção de alimentos</span><span class="sxs-lookup"><span data-stu-id="09282-244">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="09282-245">Streaming Spark: usar o Spark no HDInsight para a criação de aplicativos de streaming em tempo real</span><span class="sxs-lookup"><span data-stu-id="09282-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="09282-246">Análise de log do site usando o Spark no HDInsight</span><span class="sxs-lookup"><span data-stu-id="09282-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="09282-247">Criar e executar aplicativos</span><span class="sxs-lookup"><span data-stu-id="09282-247">Create and run applications</span></span>
* [<span data-ttu-id="09282-248">Criar um aplicativo autônomo usando Scala</span><span class="sxs-lookup"><span data-stu-id="09282-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="09282-249">Executar trabalhos remotamente em um cluster do Spark usando Livy</span><span class="sxs-lookup"><span data-stu-id="09282-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="09282-250">Ferramentas e extensões</span><span class="sxs-lookup"><span data-stu-id="09282-250">Tools and extensions</span></span>
* [<span data-ttu-id="09282-251">Use o Plug-in de Ferramentas do HDInsight para IntelliJ IDEA para criar e enviar aplicativos Spark Scala</span><span class="sxs-lookup"><span data-stu-id="09282-251">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="09282-252">Usar o plug-in de Ferramentas do HDInsight para depurar aplicativos Spark remotamente</span><span class="sxs-lookup"><span data-stu-id="09282-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="09282-253">Usar blocos de anotações do Zeppelin com um cluster Spark no HDInsight</span><span class="sxs-lookup"><span data-stu-id="09282-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="09282-254">Usar pacotes externos com blocos de notas Jupyter</span><span class="sxs-lookup"><span data-stu-id="09282-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="09282-255">Instalar o Jupyter em seu computador e conectar-se a um cluster Spark do HDInsight</span><span class="sxs-lookup"><span data-stu-id="09282-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="09282-256">Gerenciar recursos</span><span class="sxs-lookup"><span data-stu-id="09282-256">Manage resources</span></span>
* [<span data-ttu-id="09282-257">Gerenciar os recursos de cluster do Apache Spark no Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="09282-257">Manage resources for the Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="09282-258">Rastrear e depurar trabalhos em execução em um cluster do Apache Spark no HDInsight</span><span class="sxs-lookup"><span data-stu-id="09282-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
