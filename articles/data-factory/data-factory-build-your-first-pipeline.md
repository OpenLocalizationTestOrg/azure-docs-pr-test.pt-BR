---
title: 'Tutorial do Data Factory: primeiro pipeline de dados | Microsoft Docs'
description: Este tutorial do Azure Data Factory mostra como criar e agendar um data factory que processa os dados usando o script Hive em um cluster Hadoop.
services: data-factory
keywords: tutorial do azure data factory , cluster hadoop, hive do hadoop
documentationcenter: 
author: spelluru
manager: jhubbard
editor: 
ms.assetid: 81f36c76-6e78-4d93-a3f2-0317b413f1d0
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 07/10/2017
ms.author: spelluru
ms.openlocfilehash: 08e2988d455cca21726162d9fb128e91fd51f463
ms.sourcegitcommit: 18ad9bc049589c8e44ed277f8f43dcaa483f3339
ms.translationtype: MT
ms.contentlocale: pt-BR
ms.lasthandoff: 08/29/2017
---
# <a name="tutorial-build-your-first-pipeline-to-transform-data-using-hadoop-cluster"></a><span data-ttu-id="6ce9f-104">Tutorial: crie seu primeiro pipeline para processar dados usando cluster Hadoop</span><span class="sxs-lookup"><span data-stu-id="6ce9f-104">Tutorial: Build your first pipeline to transform data using Hadoop cluster</span></span>
> [!div class="op_single_selector"]
> * [<span data-ttu-id="6ce9f-105">Visão geral e pré-requisitos</span><span class="sxs-lookup"><span data-stu-id="6ce9f-105">Overview and prerequisites</span></span>](data-factory-build-your-first-pipeline.md)
> * [<span data-ttu-id="6ce9f-106">Portal do Azure</span><span class="sxs-lookup"><span data-stu-id="6ce9f-106">Azure portal</span></span>](data-factory-build-your-first-pipeline-using-editor.md)
> * [<span data-ttu-id="6ce9f-107">Visual Studio</span><span class="sxs-lookup"><span data-stu-id="6ce9f-107">Visual Studio</span></span>](data-factory-build-your-first-pipeline-using-vs.md)
> * [<span data-ttu-id="6ce9f-108">PowerShell</span><span class="sxs-lookup"><span data-stu-id="6ce9f-108">PowerShell</span></span>](data-factory-build-your-first-pipeline-using-powershell.md)
> * [<span data-ttu-id="6ce9f-109">Modelo do Resource Manager</span><span class="sxs-lookup"><span data-stu-id="6ce9f-109">Resource Manager template</span></span>](data-factory-build-your-first-pipeline-using-arm.md)
> * [<span data-ttu-id="6ce9f-110">API REST</span><span class="sxs-lookup"><span data-stu-id="6ce9f-110">REST API</span></span>](data-factory-build-your-first-pipeline-using-rest-api.md)

<span data-ttu-id="6ce9f-111">Neste tutorial, você deve criar sua Azure Data Factory com um pipeline de dados.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-111">In this tutorial, you build your first Azure data factory with a data pipeline.</span></span> <span data-ttu-id="6ce9f-112">O pipeline transforma dados de entrada, executando o script do Hive em um cluster Azure HDInsight (Hadoop) para gerar dados de saída.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-112">The pipeline transforms input data by running Hive script on an Azure HDInsight (Hadoop) cluster to produce output data.</span></span>  

<span data-ttu-id="6ce9f-113">Este artigo fornece uma visão geral e pré-requisitos para o tutorial.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-113">This article provides overview and prerequisites for the tutorial.</span></span> <span data-ttu-id="6ce9f-114">Depois de concluir os pré-requisitos, conclua o tutorial usando uma das seguintes ferramentas/SDKs: portal do Azure, Visual Studio, PowerShell, modelo do Resource Manager ou API REST.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-114">After you complete the prerequisites, you can do the tutorial using one of the following tools/SDKs: Azure portal, Visual Studio, PowerShell, Resource Manager template, REST API.</span></span> <span data-ttu-id="6ce9f-115">Selecione uma das opções na lista suspensa no início (ou) links no final deste artigo para fazer o tutorial usando uma das seguintes opções.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-115">Select one of the options in the drop-down list at the beginning (or) links at the end of this article to do the tutorial using one of these options.</span></span>    

## <a name="tutorial-overview"></a><span data-ttu-id="6ce9f-116">Visão geral do tutorial</span><span class="sxs-lookup"><span data-stu-id="6ce9f-116">Tutorial overview</span></span>
<span data-ttu-id="6ce9f-117">Neste tutorial, você executa as seguintes etapas:</span><span class="sxs-lookup"><span data-stu-id="6ce9f-117">In this tutorial, you perform the following steps:</span></span>

1. <span data-ttu-id="6ce9f-118">Criar uma **data factory**.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-118">Create a **data factory**.</span></span> <span data-ttu-id="6ce9f-119">Um data factory pode conter um ou mais pipelines de dados que movem e transformam dados.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-119">A data factory can contain one or more data pipelines that move and transform data.</span></span> 

    <span data-ttu-id="6ce9f-120">Neste tutorial, você deve criar um pipeline na data factory.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-120">In this tutorial, you create one pipeline in the data factory.</span></span> 
2. <span data-ttu-id="6ce9f-121">Criar um **pipeline**.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-121">Create a **pipeline**.</span></span> <span data-ttu-id="6ce9f-122">Um pipeline pode ter uma ou mais atividades (exemplos: atividade de cópia, atividade de Hive do HDInsight).</span><span class="sxs-lookup"><span data-stu-id="6ce9f-122">A pipeline can have one or more activities (Examples: Copy Activity, HDInsight Hive Activity).</span></span> <span data-ttu-id="6ce9f-123">Este exemplo usa a atividade de Hive do HDInsight que executa um script do Hive em um cluster de Hadoop do HDInsight.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-123">This sample uses the HDInsight Hive activity that runs a Hive script on a HDInsight Hadoop cluster.</span></span> <span data-ttu-id="6ce9f-124">Primeiro, o script cria uma tabela que faz referência aos dados brutos de log da Web colocados no armazenamento de blobs do Azure, então, particiona os dados brutos por ano e mês.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-124">The script first creates a table that references the raw web log data stored in Azure blob storage and then partitions the raw data by year and month.</span></span>

    <span data-ttu-id="6ce9f-125">Neste tutorial, o pipeline usa a atividade do Hive para transformar dados executando uma consulta de Hive em um cluster do Hadoop do Azure HDInsight.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-125">In this tutorial, the pipeline uses the Hive Activity to transform data by running a Hive query on an Azure HDInsight Hadoop cluster.</span></span> 
3. <span data-ttu-id="6ce9f-126">Crie **serviços vinculados**.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-126">Create **linked services**.</span></span> <span data-ttu-id="6ce9f-127">Crie um serviço vinculado para vincular um armazenamento de dados ou um serviço de computação ao data factory.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-127">You create a linked service to link a data store or a compute service to the data factory.</span></span> <span data-ttu-id="6ce9f-128">Um armazenamento de dados, como o Armazenamento do Azure, armazena dados de entrada/saída de atividades no pipeline.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-128">A data store such as Azure Storage holds input/output data of activities in the pipeline.</span></span> <span data-ttu-id="6ce9f-129">Um serviço de computação, como o cluster de Hadoop do HDInsight, processa/transforma os dados.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-129">A compute service such as HDInsight Hadoop cluster processes/transforms data.</span></span>

    <span data-ttu-id="6ce9f-130">Neste tutorial, você deve criar dois serviços vinculados: **Armazenamento do Azure** e **Azure HDInsight**.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-130">In this tutorial, you create two linked services: **Azure Storage** and **Azure HDInsight**.</span></span> <span data-ttu-id="6ce9f-131">O serviço vinculado de armazenamento do Azure vincula uma conta de armazenamento do Azure que contém os dados de entrada/saída para a data factory.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-131">The Azure Storage linked service links an Azure Storage Account that holds the input/output data to the data factory.</span></span> <span data-ttu-id="6ce9f-132">O serviço vinculado do Azure HDInsight vincula um cluster Azure HDInsight que é usado para transformar dados em data factory.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-132">Azure HDInsight linked service links an Azure HDInsight cluster that is used to transform data to the data factory.</span></span> 
3. <span data-ttu-id="6ce9f-133">Criar **conjuntos de dados**de entrada e saída.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-133">Create input and output **datasets**.</span></span> <span data-ttu-id="6ce9f-134">Um conjunto de dados de entrada representa a entrada de uma atividade no pipeline e um conjunto de dados de saída representa a saída da atividade.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-134">An input dataset represents the input for an activity in the pipeline and an output dataset represents the output for the activity.</span></span>

    <span data-ttu-id="6ce9f-135">Neste tutorial, os conjuntos de dados de entrada e saída especificam locais de entrada e saída de dados no armazenamento de blobs do Azure.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-135">In this tutorial, the input and output datasets specify locations of input and output data in the Azure Blob Storage.</span></span> <span data-ttu-id="6ce9f-136">O serviço vinculado de armazenamento do Azure especifica qual conta de armazenamento do Azure é usada.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-136">The Azure Storage linked service specifies what Azure Storage Account is used.</span></span> <span data-ttu-id="6ce9f-137">Um conjunto de dados de entrada especifica o local em que os arquivos de entrada estão localizados e um conjunto de dados de saída especifica o local em que os arquivos de saída são colocados.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-137">An input dataset specifies where the input files are located and an output dataset specifies where the output files are placed.</span></span> 


<span data-ttu-id="6ce9f-138">Para obter uma visão geral detalhada do Azure Data Factory, confira o artigo [Introdução à Azure Data Factory](data-factory-introduction.md).</span><span class="sxs-lookup"><span data-stu-id="6ce9f-138">See [Introduction to Azure Data Factory](data-factory-introduction.md) article for a detailed overview of Azure Data Factory.</span></span>
  
<span data-ttu-id="6ce9f-139">Aqui está a **exibição de diagrama** do data factory de exemplo que você compila neste tutorial.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-139">Here is the **diagram view** of the sample data factory you build in this tutorial.</span></span> <span data-ttu-id="6ce9f-140">**MyFirstPipeline** tem uma atividade de Hive que consome o conjunto de dados **AzureBlobInput** como uma entrada e produz o conjunto de dados **AzureBlobOutput** como uma saída.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-140">**MyFirstPipeline** has one activity of type Hive that consumes **AzureBlobInput** dataset as an input and produces **AzureBlobOutput** dataset as an output.</span></span> 

![Exibição de diagrama no tutorial do Data Factory](media/data-factory-build-your-first-pipeline/data-factory-tutorial-diagram-view.png)


<span data-ttu-id="6ce9f-142">Neste tutorial, a pasta **inputdata** do contêiner de blobs do Azure **adfgetstarted** contém um arquivo denominado input.log.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-142">In this tutorial, **inputdata** folder of the **adfgetstarted** Azure blob container contains one file named input.log.</span></span> <span data-ttu-id="6ce9f-143">Esse arquivo de log tem entradas de três meses: janeiro, fevereiro e março de 2016.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-143">This log file has entries from three months: January, February, and March of 2016.</span></span> <span data-ttu-id="6ce9f-144">Veja as linhas de exemplo para cada mês no arquivo de entrada.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-144">Here are the sample rows for each month in the input file.</span></span> 

```
2016-01-01,02:01:09,SAMPLEWEBSITE,GET,/blogposts/mvc4/step2.png,X-ARR-LOG-ID=2ec4b8ad-3cf0-4442-93ab-837317ece6a1,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,53175,871 
2016-02-01,02:01:10,SAMPLEWEBSITE,GET,/blogposts/mvc4/step7.png,X-ARR-LOG-ID=d7472a26-431a-4a4d-99eb-c7b4fda2cf4c,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,30184,871
2016-03-01,02:01:10,SAMPLEWEBSITE,GET,/blogposts/mvc4/step7.png,X-ARR-LOG-ID=d7472a26-431a-4a4d-99eb-c7b4fda2cf4c,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,30184,871
```

<span data-ttu-id="6ce9f-145">Quando o arquivo é processado pelo pipeline com a Atividade do Hive do HDInsight, a atividade executa um script do Hive no cluster do HDInsight que particiona dados de entrada por ano e por mês.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-145">When the file is processed by the pipeline with HDInsight Hive Activity, the activity runs a Hive script on the HDInsight cluster that partitions input data by year and month.</span></span> <span data-ttu-id="6ce9f-146">O script cria três pastas de saída com um arquivo com entradas de cada mês.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-146">The script creates three output folders that contain a file with entries from each month.</span></span>  

```
adfgetstarted/partitioneddata/year=2016/month=1/000000_0
adfgetstarted/partitioneddata/year=2016/month=2/000000_0
adfgetstarted/partitioneddata/year=2016/month=3/000000_0
```

<span data-ttu-id="6ce9f-147">Das linhas de exemplo mostradas acima, a primeira (com 2016-01-01) é gravada no arquivo 000000_0 na pasta month=1.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-147">From the sample lines shown above, the first one (with 2016-01-01) is written to the 000000_0 file in the month=1 folder.</span></span> <span data-ttu-id="6ce9f-148">Da mesma forma, a segunda é gravada no arquivo na pasta do mês = 2 e a terceira é gravada no arquivo na pasta do mês = 3.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-148">Similarly, the second one is written to the file in the month=2 folder and the third one is written to the file in the month=3 folder.</span></span>  

## <a name="prerequisites"></a><span data-ttu-id="6ce9f-149">Pré-requisitos</span><span class="sxs-lookup"><span data-stu-id="6ce9f-149">Prerequisites</span></span>
<span data-ttu-id="6ce9f-150">Antes de iniciar este tutorial, você deverá ter os seguintes pré-requisitos:</span><span class="sxs-lookup"><span data-stu-id="6ce9f-150">Before you begin this tutorial, you must have the following prerequisites:</span></span>

1. <span data-ttu-id="6ce9f-151">**Uma assinatura do Azure** - se você não tiver uma, poderá criar uma conta de avaliação gratuita em apenas alguns minutos.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-151">**Azure subscription** - If you don't have an Azure subscription, you can create a free trial account in just a couple of minutes.</span></span> <span data-ttu-id="6ce9f-152">Consulte o artigo [Avaliação gratuita](https://azure.microsoft.com/pricing/free-trial/) para ver como você pode obter uma conta de avaliação gratuita.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-152">See the [Free Trial](https://azure.microsoft.com/pricing/free-trial/) article on how you can obtain a free trial account.</span></span>
2. <span data-ttu-id="6ce9f-153">**Armazenamento do Azure** – você usa uma conta de armazenamento do Azure padrão de uso geral para armazenar os dados neste tutorial.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-153">**Azure Storage** – You use a general-purpose standard Azure storage account for storing the data in this tutorial.</span></span> <span data-ttu-id="6ce9f-154">Se você não tiver uma conta de armazenamento do Azure padrão de uso geral, confira o artigo [Criar uma conta de armazenamento](../storage/common/storage-create-storage-account.md#create-a-storage-account).</span><span class="sxs-lookup"><span data-stu-id="6ce9f-154">If you don't have a general-purpose standard Azure storage account, see the [Create a storage account](../storage/common/storage-create-storage-account.md#create-a-storage-account) article.</span></span> <span data-ttu-id="6ce9f-155">Depois de você ter criado a conta de armazenamento, anote o **nome da conta** e a **chave de acesso**.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-155">After you have created the storage account, note down the **account name** and **access key**.</span></span> <span data-ttu-id="6ce9f-156">Consulte [Exibir, copiar e regenerar chaves de acesso de armazenamento](../storage/common/storage-create-storage-account.md#view-and-copy-storage-access-keys).</span><span class="sxs-lookup"><span data-stu-id="6ce9f-156">See [View, copy and regenerate storage access keys](../storage/common/storage-create-storage-account.md#view-and-copy-storage-access-keys).</span></span>
3. <span data-ttu-id="6ce9f-157">Baixe e leia o arquivo de consulta de Hive (**HQL**) localizado em: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql](https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql).</span><span class="sxs-lookup"><span data-stu-id="6ce9f-157">Download and review the Hive query file (**HQL**) located at: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql](https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql).</span></span> <span data-ttu-id="6ce9f-158">Essa consulta transforma os dados de entrada para gerar dados de saída.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-158">This query transforms input data to produce output data.</span></span> 
4. <span data-ttu-id="6ce9f-159">Baixe e leia o arquivo de entrada de amostra (**input.log**) localizado em: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log](https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log)</span><span class="sxs-lookup"><span data-stu-id="6ce9f-159">Download and review the sample input file (**input.log**) located at: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log](https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log)</span></span>
5. <span data-ttu-id="6ce9f-160">Crie um contêiner de blobs denominado **adfgetstarted** em seu armazenamento de blobs do Azure.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-160">Create a blob container named **adfgetstarted** in your Azure Blob Storage.</span></span> 
6. <span data-ttu-id="6ce9f-161">Execute o arquivo **partitionweblogs.hql** para a pasta **script** do contêiner **adfgetstarted**.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-161">Upload **partitionweblogs.hql** file to the **script** folder in the **adfgetstarted** container.</span></span> <span data-ttu-id="6ce9f-162">Use ferramentas como o [Gerenciador de Armazenamento do Microsoft Azure](http://storageexplorer.com/).</span><span class="sxs-lookup"><span data-stu-id="6ce9f-162">Use tools such as [Microsoft Azure Storage Explorer](http://storageexplorer.com/).</span></span> 
7. <span data-ttu-id="6ce9f-163">Carregue o arquivo **input.log** para a pasta **inputdata** do contêiner **adfgetstarted**.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-163">Upload **input.log** file to the **inputdata** folder in the **adfgetstarted** container.</span></span> 

<span data-ttu-id="6ce9f-164">Depois de concluir os pré-requisitos, selecione uma das seguintes ferramentas/SDKs para fazer o tutorial:</span><span class="sxs-lookup"><span data-stu-id="6ce9f-164">After you complete the prerequisites, select one of the following tools/SDKs to do the tutorial:</span></span> 

- [<span data-ttu-id="6ce9f-165">Portal do Azure</span><span class="sxs-lookup"><span data-stu-id="6ce9f-165">Azure portal</span></span>](data-factory-build-your-first-pipeline-using-editor.md)
- [<span data-ttu-id="6ce9f-166">Visual Studio</span><span class="sxs-lookup"><span data-stu-id="6ce9f-166">Visual Studio</span></span>](data-factory-build-your-first-pipeline-using-vs.md)
- [<span data-ttu-id="6ce9f-167">PowerShell</span><span class="sxs-lookup"><span data-stu-id="6ce9f-167">PowerShell</span></span>](data-factory-build-your-first-pipeline-using-powershell.md)
- [<span data-ttu-id="6ce9f-168">Modelo do Resource Manager</span><span class="sxs-lookup"><span data-stu-id="6ce9f-168">Resource Manager template</span></span>](data-factory-build-your-first-pipeline-using-arm.md)
- [<span data-ttu-id="6ce9f-169">API REST</span><span class="sxs-lookup"><span data-stu-id="6ce9f-169">REST API</span></span>](data-factory-build-your-first-pipeline-using-rest-api.md)

<span data-ttu-id="6ce9f-170">O portal do Azure e o Visual Studio permitem criar data factories com a GUI.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-170">Azure portal and Visual Studio provide GUI way of building your data factories.</span></span> <span data-ttu-id="6ce9f-171">Enquanto isso, as opções do PowerShell, do modelo do Resource Manager e a API REST permitem criar data factories com script/programação.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-171">Whereas, PowerShell, Resource Manager Template, and REST API options provides scripting/programming way of building your data factories.</span></span>

> [!NOTE]
> <span data-ttu-id="6ce9f-172">O pipeline de dados neste tutorial transforma os dados de entrada para gerar dados de saída.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-172">The data pipeline in this tutorial transforms input data to produce output data.</span></span> <span data-ttu-id="6ce9f-173">Ele não copia dados de um armazenamento de dados de origem para um armazenamento de dados de destino.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-173">It does not copy data from a source data store to a destination data store.</span></span> <span data-ttu-id="6ce9f-174">Para obter um tutorial sobre como copiar dados usando o Azure Data Factory, confira [Tutorial: copiar dados do armazenamento de blobs para um banco de dados SQL](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).</span><span class="sxs-lookup"><span data-stu-id="6ce9f-174">For a tutorial on how to copy data using Azure Data Factory, see [Tutorial: Copy data from Blob Storage to SQL Database](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).</span></span>
> 
> <span data-ttu-id="6ce9f-175">É possível encadear duas atividades (executar uma atividade após a outra) definindo o conjunto de dados de saída de uma atividade como o conjunto de dados de entrada da outra atividade.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-175">You can chain two activities (run one activity after another) by setting the output dataset of one activity as the input dataset of the other activity.</span></span> <span data-ttu-id="6ce9f-176">Confira [Agendamento e execução no Data Factory](data-factory-scheduling-and-execution.md) para obter informações detalhadas.</span><span class="sxs-lookup"><span data-stu-id="6ce9f-176">See [Scheduling and execution in Data Factory](data-factory-scheduling-and-execution.md) for detailed information.</span></span> 





  
